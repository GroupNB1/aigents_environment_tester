version: '3.8'

services:
  api:
    build: .
    container_name: langchain-api
    ports:
      - "8000:8000"
    volumes:
      - .:/workspaces/aigents_environment_tester
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY:-}
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama2:7b-chat}
      - LANGCHAIN_TRACING_V2=${LANGCHAIN_TRACING_V2:-false}
      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY:-}
      - DEBUG=true
    depends_on:
      - ollama
    networks:
      - llm-network
    command: ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]

  ollama:
    image: ollama/ollama:latest
    container_name: ollama-server
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    networks:
      - llm-network
    restart: unless-stopped
    # Uncomment the next line if you have GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  streamlit:
    build: .
    container_name: streamlit-demo
    ports:
      - "8501:8501"
    volumes:
      - .:/workspaces/aigents_environment_tester
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY:-}
    depends_on:
      - api
    networks:
      - llm-network
    command: ["streamlit", "run", "demo_app.py", "--server.address", "0.0.0.0", "--server.port", "8501"]

  # Optional: Add a setup service to pull Ollama models
  ollama-setup:
    image: ollama/ollama:latest
    container_name: ollama-setup
    depends_on:
      - ollama
    networks:
      - llm-network
    volumes:
      - ollama-data:/root/.ollama
    environment:
      - OLLAMA_HOST=ollama:11434
    entrypoint: >
      sh -c "
        sleep 10 &&
        ollama pull llama2:7b-chat &&
        ollama pull mistral:7b-instruct &&
        ollama pull codellama:7b-instruct &&
        echo 'Models pulled successfully!'
      "
    restart: "no"

volumes:
  ollama-data:

networks:
  llm-network:
    driver: bridge